#+STARTUP: showall
#+INTERLEAVE_PDF: ../papers/1601.01705v1.pdf

* Learning to Compose Neural Networks for Question Answering

** 简介

   作者信息: Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein

   本片论文是 [[http://naacl.org/naacl-hlt-2016/][NAACL 2016]] 的两篇最佳(长)论文之一。

** Notes for page 1
   :PROPERTIES:
   :interleave_page_note: 1
   :END:

   问题是这样的: 给定一个 "world representation" 比如说图片或一段文章，然后再给一个问题，要根据问题从 "world representation" 中得到答案

   The model translates from questions to dynamically assembled neural networks, then applies these networks to world representations(images or knowledge bases) to produce answers.

   上面这个是总体的思想: 直接从问题中得到一个动态组装起来的神经网络，然后用这个网络从 "world representation" 中得到答案，也就是说，用 'world representation' 作为输入。

   We take advantage of two largely independent lines of work: on one hand, an extensive literature on answering questions by mapping from strings to logical representations of meaning; on the other, a series of recent successes in deep neural models for image recognition and captioning.

   主要涉及了两个领域的内容
   1. 将自然文本形式变换成逻辑表示
   2. 深度神经网络在图像识别和 'image captioning' 方面的能力

   Our model has two components, trained jointly: first, a collection of neural "modules" that can be freely composed; second, a network layout predictor that assembles modules into complete deep networks tailored to each question.

   根据处理过程，模型可以分为两部分:
   1. 许多不同类型的神经网络模块
   2. 一个 "network layout predictor" ，用来将 "神经网络模块" 组装成一个完整的神经网络

   这两部分是一起训练(trained jointly)的。

   问题来了，第一部分的模型好理解，第二部分模型的输入和输出是什么呢？输入很明确，应该就是 "逻辑表示" 的问题，这个是很好进行形式化表示的，那么输出呢？应该也是一个形式化的表示，其中可能有不同 "神经网络模块" 的名称。得到这个输出后，还必须有外部程序来真正组装起最后的网络模型。但如果说这两部分是一起被训练的话……从 "network layout" 到真正的网络模型这一步， *其数学表示为何* ?而且很重要的一点，是否可微？

   训练数据的话包含三部分: (world, question, answer)，其中 (world, question) 是输入， answer 是输出。没错的吧。

** Notes for page 2
   :PROPERTIES:
   :interleave_page_note: 2
   :END:

   2015 年 Andreas 提出了一个启发式的方法来将一个 VQA 问题解构为一串子问题，比如说，"what color is the bird" 的回答被分为两步:
   1. where is the bird
   2. what color is that part of the image

   contributions of this paper(我就理解为是作者 2016 年这篇论文而不是 2015 年的论文咯)
   1. an extension and generalization of this mechanism to enable fully differentiable reasoning about more structured semantic representations

      这是在说本文的方法对 2015 年的方法所做的扩充，使得模型能够在更加结构化的语义表示上进行 *完全可微* 的推理

      你咋不上天呢？

      "by representing every entity in the universe of discourse as a feature vector, we can obtain a distribution over entities that corresponds roughly to a logical set-valued denotation"

      将 world 中的 entity 表示成特征向量，可以得到 "a distribution over entities that corresponds roughly to a logical set-valued denotation"，这其中的 "a distribution over entities" 基本上就是废话，和前文是以个意思，但 "corresponds roughly to a logical set-valued denotation" 何解？ "correspond roughly to" 的意思为 "大致相当于" ，所以关键在后面那个 "logical set-valued denotation" ，set-valued 翻译为 "集值的"，那么就是 "逻辑集值表示" 咯，这是什么鬼意思……

      "Having obtained such a distribution, existing neural approaches use it to immediately compute a weighted average of image feature and project back into a labeling decision"

      依然没懂。不过后面倒是看明白了，大意是在描述处理的过程，第一步是将 world 中的 entity 表示成向量，然后再用 "existing neural approaches" 来得到输出，并且强调针对不同的情况应该有不同的 "neural module"，这里只说了两种：
      + combining them(by analogy to conjunction and disjunction)
      + inspecting them directly(by analogy to quantification)

      总之, "Unlike their formal conterparts, they are differentiable end-to-end" ，这应该就是主要要表达的东西了。

   2. a model for learning to assemble such modules compositionally

      "Isolated modules are of limited use, they must be composed into larger structures"

      "Our goal it to automatically induce variable-free, tree-structured computation descriptors." 这句话是说，为了达成上一句话所表示的目的(将 modules 组装起来)，需要引进一个 "variable-free, tree-structured" 的计算表示，然后引进了形式语言学(formal semantics) 中的功能符号。

      比如说，将 "What cities are in Georgia" 表示成
      #+BEGIN_EXAMPLE
      (and
          find[city]
          (related[in] lookup[Georgia])
      )
      #+END_EXAMPLE

      而在本文中，作者使用一个称为 "dynamic neural module network" 的模型来做这个事情

** Notes for page 3
   :PROPERTIES:
   :interleave_page_note: 3
   :END:

   在文本被映射到逻辑表示后，基于数据库的 QA 的相关工作有不少.

   1. "supervision may be proveded either by annotated logical forms or from (world, question, answer) triple alone"
   2. "in general the set of primitive functions from which these logical forms can be assembled is fixed"

      提炼一下主干哈: the set of functions is fixed. 这是在说啊，其逻辑形式可以被组装起来的功能，只有那么几个而已，也就是说，可被模块化的功能，种类有限，只能处理一小部分情况。

      然而嘛，这当然不是问题了，"one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features or the underlying schema." 看，还是有别人在做工作缓解了这个问题的嘛

      并且，作者表示
      + 我们的模型是一个统一的框架，既能处理 "感知输入"， 也能处理 "模式输入"(所以这两者有什么区别)
      + 我们的模型是可微的，结果是连续的(而非离散的？)


   神经网络模型在 QA 上的应用也是目前令人感兴趣的话题之一:
   + Iyyer(2014), 将 QA 看作多类别分类问题
   + Bordes(2014), "attempt to embed questions and answers in a shared vector space"
   + Hermann(2015), 使用 attention 机制来从 "documents sources" 中挑选词汇(组成答案？)
   + Yin(2015), "learns a query execution model for database tables without any"
   + Grefenstette(2013)
   + Krishnamurthy and Mitchell(2013)


   当然以上方法的问题是，需要答案能够直接能够搜索得到(require that answers can be retrieved directly based on surafce linguistic features, without any intermediate computation)

   作者接着提到，自己的工作是在 CNN 在 CV 上的应用的基础上做的。

   VQA:
   + Ren(2015), Malinowski(2015), 使用 RNN 来得到图像和问题的 "deep representations"
   + Yang(2015), Xu and Saenko(2015)，使用 question 来计算图像上的 attention
   + Zhou(2015), 简单的分类模型
   + Noh(2015), dynamic parameter prediction


   "All of these models assume that a fixed computation can be performed on the image and question to compute the answer, rather than adapting the structure of computation to the question."

   类似的工作:
   + Andreas(2015),
   + Bottou(2014), Universal parser
   + Socher(2013), recursive neural networks


   不过这些工作本身还是有限制，不像本文的方法一样: "succeeds in simultaneously learning both the parameters for and struces of instance-specific neural networks"

** Notes for page 4
   :PROPERTIES:
   :interleave_page_note: 4
   :END:

   some modules operate directly on the input representation, while others also also depend on input from specific earlier modules.

   比如说， describe 这样一个 module，除了需要 world representation 作为输入外，还需要 question 中的部分信息。

   Two base types are considered in this paper are *Attention* and *Labels* .
   + Attention: a distribution over pixels or entities
   + Labels: a distribution over answers


   Parameter arguments, like the running /bird/ example in Section 2, are provided by the layout, and are used to specialize module behavior for particular lexical items.

   Ordinary inputs are the result of computation lower in the network

* Notes for page 5
  :PROPERTIES:
  :interleave_page_note: 5
  :END:
